{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## Classification - Gradient Boosting\n",
    "\n",
    "## Name: Thantham Khamyai\n",
    "\n",
    "## Student ID: 122097\n",
    "\n",
    "### ===Task===\n",
    "\n",
    "Modify the above scratch code such that:\n",
    "- Notice that we are still using max_depth = 1.  **Attempt to tweak min_samples_split, max_depth for the regression and see whether we can achieve better mse on our boston data**\n",
    "- Notice that we only write scratch code for gradient boosting for regression, add some code so that it also works for **binary classification**.  **Load the breast cancer data from sklearn and see that it works.**\n",
    "- Further change the code so that it works for **multiclass classification**.  **Load the digits data from sklearn and see that it works**\n",
    "- Put everything into class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 4 Making Gradient Boosting Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "import numpy as np\n",
    "\n",
    "class GradientBoosting:\n",
    "    \n",
    "    def __init__ (self, estimator = None , model_params={'max_depth':3, 'min_samples_split':2},\n",
    "                  n_estimators=100, is_regression = True,\n",
    "                  learning_rate=0.1):\n",
    "        \n",
    "        if estimator == None: # Just leave this, in case future we would put other regressor model\n",
    "            estimator = DecisionTreeRegressor\n",
    "    \n",
    "        self.n_estimators = n_estimators # get number of estimators\n",
    "        \n",
    "        # init model list\n",
    "        self.models = [DummyRegressor(strategy='mean')]+[estimator(**model_params) for _ in range(self.n_estimators)]\n",
    "        \n",
    "        self.learning_rate = learning_rate # alpha\n",
    "        self.is_regression = is_regression # to check if it is regression or classification\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.models[0].fit(X, y) # fit first dummy regressor\n",
    "        \n",
    "        current_model = 1 # init first boosting idx\n",
    "        \n",
    "        while current_model != len(self.models): # while not all boosters -> keep fit them\n",
    "            \n",
    "            # predict model i and keep not result output class \n",
    "            h_x = self.predict(X, self.models[:current_model], argmax_output=False)\n",
    "            \n",
    "            residual = self.residual(y, h_x) # find residual (gradient)\n",
    "            \n",
    "            self.models[current_model].fit(X, residual) # try to fit X to residual prediction\n",
    "            \n",
    "            current_model += 1 # next model\n",
    "    \n",
    "    \n",
    "    \n",
    "    def residual(self, y, h_x):\n",
    "        return y - h_x   # simple y - h(x)\n",
    "    \n",
    "    \n",
    "    def predict(self, X, estimators=None, argmax_output=True):\n",
    "        \n",
    "        if estimators == None: # if predict was used from outside\n",
    "            estimators = self.models # define all models to be predictors\n",
    "        \n",
    "        # predict H(x) by addition of first dummy with other regressor predictions * alpha\n",
    "        H_X = self.models[0].predict(X) + sum(self.learning_rate * self.models[i].predict(X) for i in range(1, len(estimators)))\n",
    "        \n",
    "        if not self.is_regression: # if this boosting is classificaiton\n",
    "            \n",
    "            H_X = np.exp(H_X) / np.sum(np.exp(H_X), axis=1, keepdims=True) # implement softmax (works with binary and multiclass)\n",
    "        \n",
    "            if argmax_output: # if predict method was used from outside\n",
    "                \n",
    "                H_X = np.argmax(H_X, axis=1) # return class of predicted\n",
    "                \n",
    "        return H_X\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor,GradientBoostingClassifier\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 1 Implementing Regression Gradient Boosting on Boston Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading Boston, lets apply out gradient boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model Mean square error:  10.917610709239431\n"
     ]
    }
   ],
   "source": [
    "model_boston = GradientBoosting(n_estimators=200, learning_rate=0.1, model_params={'max_depth':3, 'min_samples_split':2}, is_regression=True)\n",
    "model_boston.fit(X_train, y_train)\n",
    "ypred_boston = model_boston.predict(X_test)\n",
    "print(\"Our model Mean square error: \", mean_squared_error(y_test, ypred_boston))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we have tried on max depth 1 or 3 before, lets change some to max depth 5 and min split to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model Mean square error:  11.767735935158402\n"
     ]
    }
   ],
   "source": [
    "model_boston = GradientBoosting(n_estimators=200, learning_rate=0.1, model_params={'max_depth':5, 'min_samples_split':4}, is_regression=True)\n",
    "model_boston.fit(X_train, y_train)\n",
    "ypred_boston = model_boston.predict(X_test)\n",
    "print(\"Our model Mean square error: \", mean_squared_error(y_test, ypred_boston))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that there was some lower MSE after increasing max depth and min sample split. This tells us that there was some change if we try on different hyperparameter of regressors inside models.\n",
    "\n",
    "However, to reach better result, we cannot exactly find what hyperparameters can do, we need **Cross-Validation** to automatically do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklean Mean square error:  12.35891581425541\n"
     ]
    }
   ],
   "source": [
    "sklearn_boston = GradientBoostingRegressor(n_estimators=200,learning_rate = 0.1,max_depth=3, loss='ls')\n",
    "sklearn_boston.fit(X_train, y_train)\n",
    "ypred_sk_boston = sklearn_boston.predict(X_test)\n",
    "\n",
    "print(\"Sklean Mean square error: \", mean_squared_error(y_test, ypred_sk_boston))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 2 Implementing Binary Classification Gradient Boosting on Breast Cancer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For The binary classification, we can directly implement softmax because softmax can do any binary or muiticlass classification.\n",
    "\n",
    "To address this, we need to encode by onehot method for y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(y):\n",
    "    \n",
    "    y_onehot = np.zeros((y.shape[0], len(set(y))))\n",
    "    \n",
    "    for class_i in range(len(set(y))): # to encode onehot -> loop each output\n",
    "    \n",
    "        y_at_class_i = y_train == class_i # get idx which y = class_i\n",
    "    \n",
    "        y_onehot[np.where(y_at_class_i), class_i] = 1 # change onehot row to be 1 at column class_i\n",
    "    \n",
    "    return y_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply implement onehot function to y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert binary class to One hot {0, 1}\n",
      "y train as : [[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "y_train_onehot = onehot(y_train)\n",
    "\n",
    "print('Convert binary class to One hot', set(y_train))\n",
    "print('y train as :', y_train_onehot[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we don't need to convert y_test because predictions will be integer directly, and we can examine result via integer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.95      0.91        76\n",
      "           1       0.95      0.88      0.92        95\n",
      "\n",
      "    accuracy                           0.91       171\n",
      "   macro avg       0.91      0.92      0.91       171\n",
      "weighted avg       0.92      0.91      0.91       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_cancer = GradientBoosting(n_estimators=200, learning_rate=0.1, model_params={'max_depth':3}, is_regression=False)\n",
    "model_cancer.fit(X_train, y_train_onehot)\n",
    "yhat_cancer = model_cancer.predict(X_test)\n",
    "\n",
    "# #print metrics\n",
    "print(\"Our classification report:\\n \", classification_report(y_test, yhat_cancer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94        76\n",
      "           1       0.96      0.95      0.95        95\n",
      "\n",
      "    accuracy                           0.95       171\n",
      "   macro avg       0.95      0.95      0.95       171\n",
      "weighted avg       0.95      0.95      0.95       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sklearn_cancer = GradientBoostingClassifier(n_estimators=200,learning_rate = 0.1, max_depth=1)\n",
    "sklearn_cancer.fit(X_train, y_train)\n",
    "yhat_sk_cancer = sklearn_cancer.predict(X_test)\n",
    "print(\"Sklearn classification report:\\n \", classification_report(y_test, yhat_sk_cancer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 3 Implementing Multiclass Gradient Boosting on Digits Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert binary class to One hot {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "y train as : [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=0.3, random_state=48)\n",
    "\n",
    "y_train_onehot = onehot(y_train)\n",
    "print('Convert binary class to One hot', set(y_train))\n",
    "print('y train as :', y_train_onehot[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot was performed the same as binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95        56\n",
      "           1       0.97      0.91      0.94        69\n",
      "           2       0.98      0.96      0.97        52\n",
      "           3       0.98      0.94      0.96        47\n",
      "           4       0.97      0.95      0.96        63\n",
      "           5       0.88      0.93      0.90        40\n",
      "           6       0.97      0.98      0.98        64\n",
      "           7       0.94      0.98      0.96        52\n",
      "           8       0.87      0.82      0.84        55\n",
      "           9       0.80      0.93      0.86        42\n",
      "\n",
      "    accuracy                           0.94       540\n",
      "   macro avg       0.93      0.93      0.93       540\n",
      "weighted avg       0.94      0.94      0.94       540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_digit = GradientBoosting(n_estimators=200, learning_rate=0.1, is_regression=False)\n",
    "model_digit.fit(X_train, y_train_onehot)\n",
    "yhat_digit = model_digit.predict(X_test)\n",
    "\n",
    "# #print metrics\n",
    "print(\"Our classification report:\\n \", classification_report(y_test, yhat_digit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        56\n",
      "           1       0.93      0.96      0.94        69\n",
      "           2       1.00      1.00      1.00        52\n",
      "           3       0.98      0.98      0.98        47\n",
      "           4       0.97      0.97      0.97        63\n",
      "           5       0.90      0.90      0.90        40\n",
      "           6       0.98      0.94      0.96        64\n",
      "           7       0.98      0.98      0.98        52\n",
      "           8       0.93      0.93      0.93        55\n",
      "           9       0.91      0.95      0.93        42\n",
      "\n",
      "    accuracy                           0.96       540\n",
      "   macro avg       0.96      0.96      0.96       540\n",
      "weighted avg       0.96      0.96      0.96       540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sklearn_digit = GradientBoostingClassifier(n_estimators=200,learning_rate = 0.1,max_depth=1)\n",
    "\n",
    "sklearn_digit.fit(X_train, y_train)\n",
    "yhat_sk_digit = sklearn_digit.predict(X_test)\n",
    "print(\"Sklearn classification report:\\n \", classification_report(y_test, yhat_sk_digit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
